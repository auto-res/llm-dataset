{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanakatouma/vscode/llm-dataset/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.llmdataset import main\n",
    "\n",
    "subset = None\n",
    "#dataset_name=\"ChilleD/MultiArith\"\n",
    "#dataset_name=\"gsm8k\"\n",
    "#subset='main'\n",
    "#dataset_name='lukaemon/bbh'\n",
    "#subset='boolean_expressions'\n",
    "#subset='causal_judgement'\n",
    "#dataset_name=\"reasoning-machines/gsm-hard\"\n",
    "#dataset_name = \"wikitablequestions\"\n",
    "#dataset_name = \"ChilleD/StrategyQA\"\n",
    "#dataset_name = \"allenai/ai2_arc\"\n",
    "#subset = \"ARC-Challenge\"\n",
    "#dataset_name = \"tasksource/bigbench\"\n",
    "#subset = \"abstract_narrative_understanding\"\n",
    "#dataset_name=\"aqua_rat\"\n",
    "#subset = \"raw\"\n",
    "#dataset_name=\"Idavidrein/gpqa\"\n",
    "#subset = \"gpqa_diamond\"\n",
    "#dataset_name=\"ChilleD/SVAMP\"\n",
    "dataset_name=\"tau/commonsense_qa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 7.39k/7.39k [00:00<00:00, 24.0MB/s]\n",
      "Downloading data: 100%|██████████| 1.25M/1.25M [00:02<00:00, 602kB/s]\n",
      "Downloading data: 100%|██████████| 160k/160k [00:01<00:00, 102kB/s]\n",
      "Downloading data: 100%|██████████| 151k/151k [00:01<00:00, 101kB/s]\n",
      "Generating train split: 100%|██████████| 9741/9741 [00:00<00:00, 585851.75 examples/s]\n",
      "Generating validation split: 100%|██████████| 1221/1221 [00:00<00:00, 253627.44 examples/s]\n",
      "Generating test split: 100%|██████████| 1140/1140 [00:00<00:00, 229372.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types:['train', 'validation', 'test']\n",
      "Number of data:[9741, 1221, 1140]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llmdataset = main.LLMdataset(dataset_name=dataset_name, subset=subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = llmdataset.dataloader(data_type='train', batch_size=4, seed=3655, max_data=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "[['Though the drawing of the object was ambiguous at best, the people playing the drawing game were able to identify the item because it was what?', {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['certain', 'distinct', 'common', 'known', 'specific']}, 'C'], [\"I went to eat in a restaurant, but I left because the person at the front wasn't what?\", {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['drinking wine', 'make reservations', 'polite', 'spit on me', 'prepared to pay']}, 'C'], ['Most humans by shoes in pairs, this is because they happen to have what?', {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['two ears', 'lay eggs', 'one head', 'a bad day', 'two feet']}, 'E'], ['What will happen to skin pinched in something?', {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['feel cold', 'feel pressure', 'feel bumpy', 'itches', 'feel clammy']}, 'B']]\n",
      "------\n",
      "[['James loves judging things, but all he has to go on is his own what?', {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['misperception', 'jury', 'knowing yourself', 'ignorance', 'experience']}, 'E']]\n"
     ]
    }
   ],
   "source": [
    "for i in dataloader:\n",
    "    print('------')\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"gsm8k\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ChilleD/MultiArith\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'final_ans'],\n",
       "        num_rows: 420\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'final_ans'],\n",
       "        num_rows: 180\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d5126c2dc43a469832e9ed440942492997e91558662f09f9eb531957d30c5f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
